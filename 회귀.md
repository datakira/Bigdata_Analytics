```python
!pip install statsmodels
```

    Collecting statsmodels
      Downloading statsmodels-0.12.1-cp38-none-win_amd64.whl (9.2 MB)
    Requirement already satisfied: numpy>=1.15 in c:\users\user\anaconda3\lib\site-packages (from statsmodels) (1.19.2)
    Requirement already satisfied: scipy>=1.1 in c:\users\user\anaconda3\lib\site-packages (from statsmodels) (1.5.2)
    Collecting patsy>=0.5
      Downloading patsy-0.5.1-py2.py3-none-any.whl (231 kB)
    Requirement already satisfied: pandas>=0.21 in c:\users\user\anaconda3\lib\site-packages (from statsmodels) (1.1.4)
    Requirement already satisfied: six in c:\users\user\anaconda3\lib\site-packages (from patsy>=0.5->statsmodels) (1.15.0)
    Requirement already satisfied: python-dateutil>=2.7.3 in c:\users\user\anaconda3\lib\site-packages (from pandas>=0.21->statsmodels) (2.8.1)
    Requirement already satisfied: pytz>=2017.2 in c:\users\user\anaconda3\lib\site-packages (from pandas>=0.21->statsmodels) (2020.1)
    Installing collected packages: patsy, statsmodels
    Successfully installed patsy-0.5.1 statsmodels-0.12.1
    


```python
import numpy as np
import pandas as pd
from scipy import stats
from matplotlib import pyplot as plt
import seaborn as sns
sns.set()
import statsmodels.formula.api as smf
import statsmodels.api as sm
```


```python
# 데이터 읽기
beer = pd.read_csv("./dataset/7_1_beer.csv")
print(beer.head())
```

       beer  temperature
    0  45.3         20.5
    1  59.3         25.0
    2  40.4         10.0
    3  38.0         26.9
    4  37.0         15.8
    


```python
sns.jointplot(x='temperature',y='beer',kind='reg',
             data=beer, color='skyblue')
```




    <seaborn.axisgrid.JointGrid at 0x27c1a386a90>




    
![png](%ED%9A%8C%EA%B7%80_guide_files/%ED%9A%8C%EA%B7%80_guide_3_1.png)
    



```python
# 모델 구축 : 종속변수 맥주 매상, 독립변수 기온 을 사용한 정규선형모델
# 모집단분포가 정규분포임을 가정 시 최대우도법의 결과는 최소제곱법의 결과와 일치
# 범용최소제곱법
lm_model = smf.ols(formula = "beer ~ temperature",
                  data=beer).fit()
```


```python
lm_model.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>beer</td>       <th>  R-squared:         </th> <td>   0.504</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.486</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   28.45</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sat, 14 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>1.11e-05</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:39:39</td>     <th>  Log-Likelihood:    </th> <td> -102.45</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    30</td>      <th>  AIC:               </th> <td>   208.9</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    28</td>      <th>  BIC:               </th> <td>   211.7</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>   <td>   34.6102</td> <td>    3.235</td> <td>   10.699</td> <td> 0.000</td> <td>   27.984</td> <td>   41.237</td>
</tr>
<tr>
  <th>temperature</th> <td>    0.7654</td> <td>    0.144</td> <td>    5.334</td> <td> 0.000</td> <td>    0.471</td> <td>    1.059</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.587</td> <th>  Durbin-Watson:     </th> <td>   1.960</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.746</td> <th>  Jarque-Bera (JB):  </th> <td>   0.290</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.240</td> <th>  Prob(JB):          </th> <td>   0.865</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.951</td> <th>  Cond. No.          </th> <td>    52.5</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.




```python
df = pd.read_pickle('./dataset/auto-mpg.pkl')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mpg</th>
      <th>cylinders</th>
      <th>displacement</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>model year</th>
      <th>origin</th>
      <th>car name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18.0</td>
      <td>8</td>
      <td>307.0</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>chevrolet chevelle malibu</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>8</td>
      <td>350.0</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
      <td>buick skylark 320</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.0</td>
      <td>8</td>
      <td>318.0</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
      <td>plymouth satellite</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.0</td>
      <td>8</td>
      <td>304.0</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>amc rebel sst</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.0</td>
      <td>8</td>
      <td>302.0</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
      <td>ford torino</td>
    </tr>
  </tbody>
</table>
</div>




```python
sns.jointplot(x='weight',y='mpg',kind='reg',
             data=df, color='skyblue')
```




    <seaborn.axisgrid.JointGrid at 0x27c1a7f3160>




    
![png](%ED%9A%8C%EA%B7%80_guide_files/%ED%9A%8C%EA%B7%80_guide_7_1.png)
    



```python
lm_model = smf.ols(formula = "mpg ~ weight",
                  data=df).fit()
```


```python
lm_model.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.693</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.692</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   878.8</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Sat, 14 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>6.02e-102</td>
</tr>
<tr>
  <th>Time:</th>                 <td>16:09:43</td>     <th>  Log-Likelihood:    </th> <td> -1130.0</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2264.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   390</td>      <th>  BIC:               </th> <td>   2272.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   46.2165</td> <td>    0.799</td> <td>   57.867</td> <td> 0.000</td> <td>   44.646</td> <td>   47.787</td>
</tr>
<tr>
  <th>weight</th>    <td>   -0.0076</td> <td>    0.000</td> <td>  -29.645</td> <td> 0.000</td> <td>   -0.008</td> <td>   -0.007</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>41.682</td> <th>  Durbin-Watson:     </th> <td>   0.808</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  60.039</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.727</td> <th>  Prob(JB):          </th> <td>9.18e-14</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.251</td> <th>  Cond. No.          </th> <td>1.13e+04</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.13e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.


