## 차원 축소
* 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것
* 차원이 증가할 수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고 희소한 구조를 가지며 예측 신뢰도가 떨어짐
* 선형 모델에서는 입력 변수 간의 상관관계가 높을 경우 다중 공선성 문제로 모델의 예측 성능 저하
* 다차원의 피처를 차원 축소해 피처 수를 줄이면 더 직관적으로 데이터를 해석할 수 있음
* 차원 축소는 피처 선택과 피처 추출로 나눌 수 있음
* 피처 선택은 특정 피처에 종속성이 강한 불필요한 피처는 제거하고 데이터의 특징을 잘 나타내는 주요 피처만 선택
* 피처 추출은 기존 피처를 저차원의 중요 피처로 압축해서 추출. 기존의 피처가 압축된 것이므로 기존 피처와는 다른 값이 됨
* 차원 축소를 통해 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하며 PCA, SVD, NMF은 대표적인 차원 축소 알고리즘

### PCA
* 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며 이렇게 분해된 고유벡터을 이용해 입력 데이터를 선형 변환하는 방식이 PCA
* 입력 데이터 세트의 공분산 행렬을 생성
* 공분산 행렬의 고유벡터와 고유값을 계산
* 고유값이 가장 큰 순으로 K개(PCA 변환 차수)만큼 고유벡터를 추출
* 고유값이 가장 큰 순으로 추출되며 고유벡터를 이용해 새롭게 입력 데이터을 변환

#### 선형대수식
* 크기와 방향을 가지고 있는 것을 vector, 반면 크기만 있는 것(질량, 온도, 길이)을 scalar이라고 한다
* 고유벡터 : 행렬 A를 곱하더라도 방향이 변하지 않고 그 크기만 변하는 벡터 Ax = ax(A 행렬, x 고유벡터, a 스칼라)
* 고유값(Eigenvalue) : 고유 벡터의 크기. 입력데이터의 분산
* 선형 변환 : 특정 벡터에 행렬  A를 곱해 새로운 벡터로 변환
* 공분산 행렬 : 두변수간의 변동을 의미. 공분산 Cov(X,Y) > 0 은 X가 증가할 때 Y도 증가
* 대칭행렬 : 고유벡터를 항상 직교행렬로 고유값을 정방 행렬로 대각화할 수 있음
* 직교행렬 : 대각원소 이외의 모든 원소가 0일때. 직교행렬의 역행렬은 직교행렬 자신의 전치행렬과 같음

#### LDA(Linear Discriminant Analysis)
* PCA와 매우 유사한 방식이며 PCA가 입력 데이터 변동성의 가장 큰 축을 찾는데 반해 LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾는 방식으로 차원을 축소

#### SVD, NMF
* 매우 많은 피처 데이터를 가진 고차원 행렬을 두 개의 저차원 행렬로 분리하는 행렬 분해 기법.
* 행렬 분해를 수행하면서 원본 행렬에서 잠재된 요소를 추출하기 때문에 토픽 모델이나 추천 시스템에서 활발하게 사용 


```python
# PCA 차원으로 압축
from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt

iris = load_iris()
# 넘파이 데이터 세트를 판다스 DataFrame으로 변환
columns = ['sepal_length','sepal_width','petal_length','petal_width']
df_iris = pd.DataFrame(iris.data, columns=columns)
df_iris['target']=iris.target
df_iris.head()

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal_length</th>
      <th>sepal_width</th>
      <th>petal_length</th>
      <th>petal_width</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
iris.keys()
# iris.target
iris.target_names
# iris.feature_names
```




    array(['setosa', 'versicolor', 'virginica'], dtype='<U10')




```python
# sepal_length와 sepal_width를 x축, y축으로 해 품종 데이터 분포를 시각화

markers = ['^','s','o']
for i, marker in enumerate(markers):
    x_axis_data = df_iris[df_iris.target==i]['sepal_length']
    y_axis_data = df_iris[df_iris.target==i]['sepal_width']
    plt.scatter(x_axis_data,y_axis_data,marker=marker,label=iris.target_names[i])
plt.legend()
plt.xlabel('sepal_length')
plt.ylabel('sepa_width')
```




    Text(0, 0.5, 'sepa_width')




    
![png](%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C_guide_files/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C_guide_4_1.png)
    



```python
# PCA로 4개 속성을 2개로 압축
# StandardScaler를 이용 평균 0, 분산이 1인 표준 정규분포로 모든 속성값 변환
from sklearn.preprocessing import StandardScaler
iris_scaled = StandardScaler().fit_transform(df_iris)
iris_scaled[:3]
```




    array([[-0.90068117,  1.01900435, -1.34022653, -1.3154443 , -1.22474487],
           [-1.14301691, -0.13197948, -1.34022653, -1.3154443 , -1.22474487],
           [-1.38535265,  0.32841405, -1.39706395, -1.3154443 , -1.22474487]])




```python
# 생성 파라미터로 n_components를 입력 받음. PCA로 변환할 차원의 수
from sklearn.decomposition import PCA

pca = PCA(n_components=2)

iris_pca = pca.fit_transform(iris_scaled)
iris_pca.shape
```




    (150, 2)




```python
pca_columns = ['pca_component_1','pca_component_2']
df_iris_pca = pd.DataFrame(iris_pca,columns=pca_columns)
df_iris_pca['target']=iris.target
df_iris_pca.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pca_component_1</th>
      <th>pca_component_2</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-2.576120</td>
      <td>0.474499</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-2.415322</td>
      <td>-0.678092</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-2.659333</td>
      <td>-0.348282</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-2.601991</td>
      <td>-0.603306</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-2.683744</td>
      <td>0.640220</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
markers = ['^','s','o']
for i, marker in enumerate(markers):
    x_axis_data = df_iris_pca[df_iris_pca.target==i]['pca_component_1']
    y_axis_data = df_iris_pca[df_iris_pca.target==i]['pca_component_2']
    plt.scatter(x_axis_data,y_axis_data,marker=marker,label=iris.target_names[i])
plt.legend()
plt.xlabel('pca_component_1')
plt.ylabel('pca_component_2')
```




    Text(0, 0.5, 'pca_component_2')




    
![png](%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C_guide_files/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C_guide_8_1.png)
    



```python
# 전체 변동성에서 개별 PCA 컴포넌트별로 차지하는 변동성 비율 : explained_variance_ratio_
# 2개 pca 요소로 원본 데이터의 변동성을 95% 설명
pca.explained_variance_ratio_
```




    array([0.76740358, 0.18282727])




```python
# 원본 데이터
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rcf = RandomForestClassifier(random_state=156)
scores=cross_val_score(rcf,iris.data, iris.target,scoring='accuracy',cv=3)
print('cv=3인 경우의 개별 폴드 세트별 정확도:', scores)
print('평균정확도:{:.2f}'.format(np.mean(scores)))
```

    cv=3인 경우의 개별 폴드 세트별 정확도: [0.98 0.94 0.96]
    평균정확도:0.96
    


```python
# PCA 변환 데이터 
pca_X=df_iris_pca[['pca_component_1','pca_component_2']]
scores_pca = cross_val_score(rcf, pca_X,iris.target,scoring='accuracy',cv=3)
print('cv=3인 경우의 개별 폴드 세트별 정확도:',scores_pca)
print('평균정확도:{:.2f}'.format(np.mean(scores_pca)))
```

    cv=3인 경우의 개별 폴드 세트별 정확도: [0.98 0.98 1.  ]
    평균정확도:0.99
    

### PCA 데이터 세트에 기반한 신용카드 고객 연체 여부 분류 예측
* 신용카드 데이터 세트는 30000개의 레코드와 24개의 속성을 가지고 있으며 'default payment next month' 속성이 
  Target 값으로 연체일 경우 1, 정상납부가 0임.


```python
import pandas as pd

df = pd.read_excel('./dataset/credit_card.xls', sheet_name='Data',header=1)
print(df.shape)
df.head()
```

    (30000, 25)
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>LIMIT_BAL</th>
      <th>SEX</th>
      <th>EDUCATION</th>
      <th>MARRIAGE</th>
      <th>AGE</th>
      <th>PAY_0</th>
      <th>PAY_2</th>
      <th>PAY_3</th>
      <th>PAY_4</th>
      <th>...</th>
      <th>BILL_AMT4</th>
      <th>BILL_AMT5</th>
      <th>BILL_AMT6</th>
      <th>PAY_AMT1</th>
      <th>PAY_AMT2</th>
      <th>PAY_AMT3</th>
      <th>PAY_AMT4</th>
      <th>PAY_AMT5</th>
      <th>PAY_AMT6</th>
      <th>default payment next month</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>20000</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>24</td>
      <td>2</td>
      <td>2</td>
      <td>-1</td>
      <td>-1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>689</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>120000</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>26</td>
      <td>-1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>3272</td>
      <td>3455</td>
      <td>3261</td>
      <td>0</td>
      <td>1000</td>
      <td>1000</td>
      <td>1000</td>
      <td>0</td>
      <td>2000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>90000</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>34</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>14331</td>
      <td>14948</td>
      <td>15549</td>
      <td>1518</td>
      <td>1500</td>
      <td>1000</td>
      <td>1000</td>
      <td>1000</td>
      <td>5000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>50000</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>37</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>28314</td>
      <td>28959</td>
      <td>29547</td>
      <td>2000</td>
      <td>2019</td>
      <td>1200</td>
      <td>1100</td>
      <td>1069</td>
      <td>1000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>50000</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>57</td>
      <td>-1</td>
      <td>0</td>
      <td>-1</td>
      <td>0</td>
      <td>...</td>
      <td>20940</td>
      <td>19146</td>
      <td>19131</td>
      <td>2000</td>
      <td>36681</td>
      <td>10000</td>
      <td>9000</td>
      <td>689</td>
      <td>679</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 25 columns</p>
</div>




```python
# PAY_0 칼럼을 PAY_1으로 "default payment next month"을 "default"로 변경
df.rename(columns={'PAY_0':'PAY_1','default payment next month':'default'},
          inplace=True)
y_target=df['default']
X_features=df.drop(['default','ID'],axis=1)
```


```python
# 23개 속성 데이터가 있으마 속성간 상관도가 높을 수 있으므로 corr()를 이용해
# 속성간 상관도 구한 뒤 시본으로 시각화하여 확인
import seaborn as sns
import matplotlib.pyplot as plt

corr = X_features.corr()
plt.figure(figsize=(14,14))
sns.heatmap(corr,annot=True,fmt='.1g')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2810009ae80>




    
![png](%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C_guide_files/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C_guide_15_1.png)
    



```python
# 상관도가 높은 속성을 pca로 변환한 뒤 explained_variance_ratio_ 속성으로 확인
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# BILL_AMT1 ~ BILL_AMT6까지 6개의 속성명 생성
cols_bill = ['BILL_AMT'+str(i) for i in range(1,7)]
print('대상 속성명:', cols_bill)

# 6개의 속성을 2개의 컴포넌트로 pca 변환하고 변동성을 알아봄
scaler = StandardScaler()
df_cols_scaled = scaler.fit_transform(X_features[cols_bill])
pca = PCA(n_components=2)
pca.fit(df_cols_scaled)
print('PCA Component별 변동성:' ,pca.explained_variance_ratio_)
```

    대상 속성명: ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']
    PCA Component별 변동성: [0.90555253 0.0509867 ]
    


```python
rcf = RandomForestClassifier(n_estimators=300, random_state=150)
scores=cross_val_score(rcf, X_features, y_target, scoring='accuracy',cv=3)
print('CV=3인 경우의 개별 FOLD세트별 정확도:', scores)
print('평균정확도:{0:.4f}'.format(np.mean(scores)))
```

    CV=3인 경우의 개별 FOLD세트별 정확도: [0.8067 0.8214 0.8225]
    평균정확도:{0:.4f} 0.8168666666666665
    


```python
scaler = StandardScaler()
df_scaled = scaler.fit_transform(X_features)

pca = PCA(n_components=6)
df_pca = pca.fit_transform(df_scaled)
scores_pca = cross_val_score(rcf, df_pca, y_target,scoring='accuracy',cv=3)
print('CV=3인 경우의 PCA 변환된 개별 Fold 세트별 정확도:',scores_pca)
print('PCA 변환 데이터 세트 평균 정확도:{0:.4f}'.format(np.mean(scores_pca)))

```

    CV=3인 경우의 PCA 변환된 개별 Fold 세트별 정확도: [0.7915 0.7972 0.8027]
    PCA 변환 데이터 세트 평균 정확도:0.7971
    


```python
pca.explained_variance_ratio_
```




    array([0.28448215, 0.17818817, 0.06743307, 0.06401149, 0.04457534,
           0.04161689])


